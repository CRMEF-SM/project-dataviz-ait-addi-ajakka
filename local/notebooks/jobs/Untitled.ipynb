{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "684a9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-95dbaaf2-e4df-4b52-83f2-d227358179d3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.11;2.4.4 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.0.0 in central\n",
      "\tfound org.lz4#lz4-java;1.4.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.3 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.11/2.4.4/spark-sql-kafka-0-10_2.11-2.4.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.11;2.4.4!spark-sql-kafka-0-10_2.11.jar (1302ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.0.0/kafka-clients-2.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.0.0!kafka-clients.jar (4604ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (79ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.4.0!lz4-java.jar (648ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.3!snappy-java.jar(bundle) (3014ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (118ms)\n",
      ":: resolution report :: resolve 22524ms :: artifacts dl 9789ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.kafka#kafka-clients;2.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.11;2.4.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.4.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   6   |   6   |   0   ||   6   |   6   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-95dbaaf2-e4df-4b52-83f2-d227358179d3\n",
      "\tconfs: [default]\n",
      "\t6 artifacts copied, 0 already retrieved (4749kB/82ms)\n",
      "22/12/20 21:59:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/12/20 21:59:23 INFO SparkContext: Running Spark version 2.4.5\n",
      "22/12/20 21:59:23 INFO SparkContext: Submitted application: /opt/workspace/notebooks/jobs/pi.py\n",
      "22/12/20 21:59:24 INFO SecurityManager: Changing view acls to: root\n",
      "22/12/20 21:59:24 INFO SecurityManager: Changing modify acls to: root\n",
      "22/12/20 21:59:24 INFO SecurityManager: Changing view acls groups to: \n",
      "22/12/20 21:59:24 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/12/20 21:59:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "22/12/20 21:59:25 INFO Utils: Successfully started service 'sparkDriver' on port 41383.\n",
      "22/12/20 21:59:25 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/12/20 21:59:25 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/12/20 21:59:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/12/20 21:59:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/12/20 21:59:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-59945384-0b2a-4879-8643-09b0b3918d30\n",
      "22/12/20 21:59:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "22/12/20 21:59:25 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/12/20 21:59:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/12/20 21:59:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://53a5440a657d:4040\n",
      "22/12/20 21:59:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.4.jar at spark://53a5440a657d:41383/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.4.jar with timestamp 1671573567252\n",
      "22/12/20 21:59:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar at spark://53a5440a657d:41383/jars/org.apache.kafka_kafka-clients-2.0.0.jar with timestamp 1671573567255\n",
      "22/12/20 21:59:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://53a5440a657d:41383/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1671573567256\n",
      "22/12/20 21:59:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar at spark://53a5440a657d:41383/jars/org.lz4_lz4-java-1.4.0.jar with timestamp 1671573567257\n",
      "22/12/20 21:59:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar at spark://53a5440a657d:41383/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar with timestamp 1671573567270\n",
      "22/12/20 21:59:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://53a5440a657d:41383/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1671573567271\n",
      "22/12/20 21:59:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.4.jar at spark://53a5440a657d:41383/files/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.4.jar with timestamp 1671573567436\n",
      "22/12/20 21:59:27 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.4.jar to /tmp/spark-6c6190f4-abdb-4777-b8d9-dab6ddc17191/userFiles-03e60299-50b3-423c-bb10-aeaf144c533d/org.apache.spark_spark-sql-kafka-0-10_2.11-2.4.4.jar\n",
      "22/12/20 21:59:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar at spark://53a5440a657d:41383/files/org.apache.kafka_kafka-clients-2.0.0.jar with timestamp 1671573567494\n",
      "22/12/20 21:59:27 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar to /tmp/spark-6c6190f4-abdb-4777-b8d9-dab6ddc17191/userFiles-03e60299-50b3-423c-bb10-aeaf144c533d/org.apache.kafka_kafka-clients-2.0.0.jar\n",
      "22/12/20 21:59:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://53a5440a657d:41383/files/org.spark-project.spark_unused-1.0.0.jar with timestamp 1671573567540\n",
      "22/12/20 21:59:27 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-6c6190f4-abdb-4777-b8d9-dab6ddc17191/userFiles-03e60299-50b3-423c-bb10-aeaf144c533d/org.spark-project.spark_unused-1.0.0.jar\n",
      "22/12/20 21:59:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar at spark://53a5440a657d:41383/files/org.lz4_lz4-java-1.4.0.jar with timestamp 1671573567558\n",
      "22/12/20 21:59:27 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar to /tmp/spark-6c6190f4-abdb-4777-b8d9-dab6ddc17191/userFiles-03e60299-50b3-423c-bb10-aeaf144c533d/org.lz4_lz4-java-1.4.0.jar\n",
      "22/12/20 21:59:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar at spark://53a5440a657d:41383/files/org.xerial.snappy_snappy-java-1.1.7.3.jar with timestamp 1671573567601\n",
      "22/12/20 21:59:27 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar to /tmp/spark-6c6190f4-abdb-4777-b8d9-dab6ddc17191/userFiles-03e60299-50b3-423c-bb10-aeaf144c533d/org.xerial.snappy_snappy-java-1.1.7.3.jar\n",
      "22/12/20 21:59:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://53a5440a657d:41383/files/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1671573567664\n",
      "22/12/20 21:59:27 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-6c6190f4-abdb-4777-b8d9-dab6ddc17191/userFiles-03e60299-50b3-423c-bb10-aeaf144c533d/org.slf4j_slf4j-api-1.7.16.jar\n",
      "22/12/20 21:59:28 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "22/12/20 21:59:28 INFO TransportClientFactory: Successfully created connection to spark-master/172.20.0.3:7077 after 142 ms (0 ms spent in bootstraps)\n",
      "22/12/20 21:59:29 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20221220215929-0000\n",
      "22/12/20 21:59:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42801.\n",
      "22/12/20 21:59:29 INFO NettyBlockTransferService: Server created on 53a5440a657d:42801\n",
      "22/12/20 21:59:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/12/20 21:59:29 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20221220215929-0000/0 on worker-20221220214312-172.20.0.4-40111 (172.20.0.4:40111) with 1 core(s)\n",
      "22/12/20 21:59:29 INFO StandaloneSchedulerBackend: Granted executor ID app-20221220215929-0000/0 on hostPort 172.20.0.4:40111 with 1 core(s), 512.0 MB RAM\n",
      "22/12/20 21:59:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20221220215929-0000/1 on worker-20221220214312-172.20.0.5-32845 (172.20.0.5:32845) with 1 core(s)\n",
      "22/12/20 21:59:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20221220215929-0000/1 on hostPort 172.20.0.5:32845 with 1 core(s), 512.0 MB RAM\n",
      "22/12/20 21:59:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 53a5440a657d, 42801, None)\n",
      "22/12/20 21:59:30 INFO BlockManagerMasterEndpoint: Registering block manager 53a5440a657d:42801 with 366.3 MB RAM, BlockManagerId(driver, 53a5440a657d, 42801, None)\n",
      "22/12/20 21:59:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 53a5440a657d, 42801, None)\n",
      "22/12/20 21:59:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 53a5440a657d, 42801, None)\n",
      "22/12/20 21:59:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20221220215929-0000/0 is now RUNNING\n",
      "22/12/20 21:59:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20221220215929-0000/1 is now RUNNING\n",
      "22/12/20 21:59:34 INFO EventLoggingListener: Logging events to file:/opt/workspace/events/app-20221220215929-0000\n",
      "22/12/20 21:59:34 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "22/12/20 21:59:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/workspace/notebooks/jobs/spark-warehouse').\n",
      "22/12/20 21:59:36 INFO SharedState: Warehouse path is 'file:/opt/workspace/notebooks/jobs/spark-warehouse'.\n",
      "22/12/20 21:59:39 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "22/12/20 21:59:42 INFO SparkContext: Starting job: reduce at /opt/workspace/notebooks/jobs/pi.py:39\n",
      "22/12/20 21:59:43 INFO DAGScheduler: Got job 0 (reduce at /opt/workspace/notebooks/jobs/pi.py:39) with 2 output partitions\n",
      "22/12/20 21:59:43 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at /opt/workspace/notebooks/jobs/pi.py:39)\n",
      "22/12/20 21:59:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/12/20 21:59:43 INFO DAGScheduler: Missing parents: List()\n",
      "22/12/20 21:59:43 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at /opt/workspace/notebooks/jobs/pi.py:39), which has no missing parents\n",
      "22/12/20 21:59:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.6 KB, free 366.3 MB)\n",
      "22/12/20 21:59:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.5 KB, free 366.3 MB)\n",
      "22/12/20 21:59:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 53a5440a657d:42801 (size: 4.5 KB, free: 366.3 MB)\n",
      "22/12/20 21:59:44 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1163\n",
      "22/12/20 21:59:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /opt/workspace/notebooks/jobs/pi.py:39) (first 15 tasks are for partitions Vector(0, 1))\n",
      "22/12/20 21:59:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
      "22/12/20 21:59:53 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.4:49768) with ID 0\n",
      "22/12/20 21:59:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.20.0.4, executor 0, partition 0, PROCESS_LOCAL, 7856 bytes)\n",
      "22/12/20 21:59:53 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.20.0.5:48608) with ID 1\n",
      "22/12/20 21:59:53 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 172.20.0.5, executor 1, partition 1, PROCESS_LOCAL, 7856 bytes)\n",
      "22/12/20 21:59:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.4:43521 with 93.3 MB RAM, BlockManagerId(0, 172.20.0.4, 43521, None)\n",
      "22/12/20 21:59:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.0.5:33425 with 93.3 MB RAM, BlockManagerId(1, 172.20.0.5, 33425, None)\n",
      "22/12/20 21:59:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.0.4:43521 (size: 4.5 KB, free: 93.3 MB)\n",
      "22/12/20 21:59:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.0.5:33425 (size: 4.5 KB, free: 93.3 MB)\n",
      "22/12/20 22:00:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 57070 ms on 172.20.0.4 (executor 0) (1/2)\n",
      "22/12/20 22:00:50 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 46353\n",
      "22/12/20 22:00:51 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 57485 ms on 172.20.0.5 (executor 1) (2/2)\n",
      "22/12/20 22:00:51 INFO DAGScheduler: ResultStage 0 (reduce at /opt/workspace/notebooks/jobs/pi.py:39) finished in 67.588 s\n",
      "22/12/20 22:00:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/12/20 22:00:51 INFO DAGScheduler: Job 0 finished: reduce at /opt/workspace/notebooks/jobs/pi.py:39, took 68.330840 s\n",
      "Pi is roughly 3.141714\n",
      "22/12/20 22:00:51 INFO SparkUI: Stopped Spark web UI at http://53a5440a657d:4040\n",
      "22/12/20 22:00:51 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "22/12/20 22:00:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
      "22/12/20 22:00:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "22/12/20 22:00:51 INFO MemoryStore: MemoryStore cleared\n",
      "22/12/20 22:00:51 INFO BlockManager: BlockManager stopped\n",
      "22/12/20 22:00:51 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "22/12/20 22:00:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "22/12/20 22:00:51 INFO SparkContext: Successfully stopped SparkContext\n",
      "22/12/20 22:00:52 INFO ShutdownHookManager: Shutdown hook called\n",
      "22/12/20 22:00:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-afd6e634-2020-4cd3-a41f-6a9e472d8bf8\n",
      "22/12/20 22:00:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-6c6190f4-abdb-4777-b8d9-dab6ddc17191/pyspark-0b53bdc7-de15-4a39-aa90-c582ee5091b8\n",
      "22/12/20 22:00:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-6c6190f4-abdb-4777-b8d9-dab6ddc17191\n"
     ]
    }
   ],
   "source": [
    "!./spark-submit.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
