{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x spark-submit.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b088b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.bahir#spark-streaming-mqtt_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-90e43ae2-93a6-42e8-afe2-3552a88db2a5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.bahir#spark-streaming-mqtt_2.11;2.4.0 in central\n",
      "\tfound org.eclipse.paho#org.eclipse.paho.client.mqttv3;1.1.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 317ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.bahir#spark-streaming-mqtt_2.11;2.4.0 from central in [default]\n",
      "\torg.eclipse.paho#org.eclipse.paho.client.mqttv3;1.1.0 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-90e43ae2-93a6-42e8-afe2-3552a88db2a5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n",
      "22/12/29 21:26:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/12/29 21:26:07 INFO SparkContext: Running Spark version 2.4.5\n",
      "22/12/29 21:26:07 INFO SparkContext: Submitted application: mqtt.py\n",
      "22/12/29 21:26:07 INFO SecurityManager: Changing view acls to: root\n",
      "22/12/29 21:26:07 INFO SecurityManager: Changing modify acls to: root\n",
      "22/12/29 21:26:07 INFO SecurityManager: Changing view acls groups to: \n",
      "22/12/29 21:26:07 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/12/29 21:26:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "22/12/29 21:26:07 INFO Utils: Successfully started service 'sparkDriver' on port 45155.\n",
      "22/12/29 21:26:07 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/12/29 21:26:07 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/12/29 21:26:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/12/29 21:26:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/12/29 21:26:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-02edeee3-6dff-4b6d-9b86-d09da5ab5f76\n",
      "22/12/29 21:26:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "22/12/29 21:26:07 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/12/29 21:26:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/12/29 21:26:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://6f8468547571:4040\n",
      "22/12/29 21:26:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.bahir_spark-streaming-mqtt_2.11-2.4.0.jar at spark://6f8468547571:45155/jars/org.apache.bahir_spark-streaming-mqtt_2.11-2.4.0.jar with timestamp 1672349168228\n",
      "22/12/29 21:26:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.eclipse.paho_org.eclipse.paho.client.mqttv3-1.1.0.jar at spark://6f8468547571:45155/jars/org.eclipse.paho_org.eclipse.paho.client.mqttv3-1.1.0.jar with timestamp 1672349168229\n",
      "22/12/29 21:26:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://6f8468547571:45155/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1672349168229\n",
      "22/12/29 21:26:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.bahir_spark-streaming-mqtt_2.11-2.4.0.jar at spark://6f8468547571:45155/files/org.apache.bahir_spark-streaming-mqtt_2.11-2.4.0.jar with timestamp 1672349168259\n",
      "22/12/29 21:26:08 INFO Utils: Copying /root/.ivy2/jars/org.apache.bahir_spark-streaming-mqtt_2.11-2.4.0.jar to /tmp/spark-6844fb75-5144-48c7-8f9a-5d6d9871a730/userFiles-d1f247ac-aa49-4ed4-a846-513f810953d2/org.apache.bahir_spark-streaming-mqtt_2.11-2.4.0.jar\n",
      "22/12/29 21:26:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.eclipse.paho_org.eclipse.paho.client.mqttv3-1.1.0.jar at spark://6f8468547571:45155/files/org.eclipse.paho_org.eclipse.paho.client.mqttv3-1.1.0.jar with timestamp 1672349168279\n",
      "22/12/29 21:26:08 INFO Utils: Copying /root/.ivy2/jars/org.eclipse.paho_org.eclipse.paho.client.mqttv3-1.1.0.jar to /tmp/spark-6844fb75-5144-48c7-8f9a-5d6d9871a730/userFiles-d1f247ac-aa49-4ed4-a846-513f810953d2/org.eclipse.paho_org.eclipse.paho.client.mqttv3-1.1.0.jar\n",
      "22/12/29 21:26:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://6f8468547571:45155/files/org.spark-project.spark_unused-1.0.0.jar with timestamp 1672349168284\n",
      "22/12/29 21:26:08 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-6844fb75-5144-48c7-8f9a-5d6d9871a730/userFiles-d1f247ac-aa49-4ed4-a846-513f810953d2/org.spark-project.spark_unused-1.0.0.jar\n",
      "22/12/29 21:26:08 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "22/12/29 21:26:08 INFO TransportClientFactory: Successfully created connection to spark-master/172.24.0.3:7077 after 45 ms (0 ms spent in bootstraps)\n",
      "22/12/29 21:26:08 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20221229212608-0000\n",
      "22/12/29 21:26:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41351.\n",
      "22/12/29 21:26:08 INFO NettyBlockTransferService: Server created on 6f8468547571:41351\n",
      "22/12/29 21:26:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/12/29 21:26:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20221229212608-0000/0 on worker-20221229212544-172.24.0.5-36827 (172.24.0.5:36827) with 1 core(s)\n",
      "22/12/29 21:26:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20221229212608-0000/0 on hostPort 172.24.0.5:36827 with 1 core(s), 1024.0 MB RAM\n",
      "22/12/29 21:26:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20221229212608-0000/1 on worker-20221229212544-172.24.0.4-36623 (172.24.0.4:36623) with 1 core(s)\n",
      "22/12/29 21:26:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20221229212608-0000/1 on hostPort 172.24.0.4:36623 with 1 core(s), 1024.0 MB RAM\n",
      "22/12/29 21:26:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20221229212608-0000/2 on worker-20221229212544-172.24.0.6-43233 (172.24.0.6:43233) with 1 core(s)\n",
      "22/12/29 21:26:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20221229212608-0000/2 on hostPort 172.24.0.6:43233 with 1 core(s), 1024.0 MB RAM\n",
      "22/12/29 21:26:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6f8468547571, 41351, None)\n",
      "22/12/29 21:26:08 INFO BlockManagerMasterEndpoint: Registering block manager 6f8468547571:41351 with 366.3 MB RAM, BlockManagerId(driver, 6f8468547571, 41351, None)\n",
      "22/12/29 21:26:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6f8468547571, 41351, None)\n",
      "22/12/29 21:26:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6f8468547571, 41351, None)\n",
      "22/12/29 21:26:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20221229212608-0000/2 is now RUNNING\n",
      "22/12/29 21:26:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20221229212608-0000/1 is now RUNNING\n",
      "22/12/29 21:26:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20221229212608-0000/0 is now RUNNING\n",
      "22/12/29 21:26:10 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "*****************************************************\n",
      "saved temp_humidity to influxDb\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "saved metric to influxDb\n",
      "*****************************************************\n",
      "saved temp_humidity to influxDb\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "saved metric to influxDb\n",
      "----------------------------------------------\n",
      "saved location to influxDb\n",
      "----------------------------------------------\n",
      "saved location to influxDb\n",
      "//////////////////////////////////////////////////\n",
      "record saved weight to influxDb\n",
      "//////////////////////////////////////////////////\n",
      "record saved weight to influxDb\n",
      "//////////////////////////////////////////////////\n",
      "record saved weight to influxDb\n",
      "//////////////////////////////////////////////////\n",
      "record saved weight to influxDb\n",
      "//////////////////////////////////////////////////\n",
      "record saved weight to influxDb\n",
      "//////////////////////////////////////////////////\n",
      "record saved weight to influxDb\n",
      "//////////////////////////////////////////////////\n",
      "record saved weight to influxDb\n"
     ]
    }
   ],
   "source": [
    "!./spark-submit.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
